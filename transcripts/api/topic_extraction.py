import re

import nltk
import spacy
from spacy.lang.en import English
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

parser = English()
en_stop = set(nltk.corpus.stopwords.words('english'))

class TokenClassification:
    def __init__(self):
        self.nlp = spacy.load('en_core_web_lg')

    def tokenize(self, data):
        """
        2.6 Entity Names Annotation

        PERSON People, including fictional
        NORP Nationalities or religious or political groups
        FACILITY Buildings, airports, highways, bridges, etc.
        ORGANIZATION Companies, agencies, institutions, etc.
        GPE Countries, cities, states
        LOCATION Non-GPE locations, mountain ranges, bodies of water
        PRODUCT Vehicles, weapons, foods, etc. (Not services)
        EVENT Named hurricanes, battles, wars, sports events, etc.
        WORK OF ART Titles of books, songs, etc.
        LAW Named documents made into laws
        LANGUAGE Any named language

        The following values are also annotated in a style similar to names:
        DATE Absolute or relative dates or periods
        TIME Times smaller than a day
        PERCENT Percentage (including “%”)
        MONEY Monetary values, including unit
        QUANTITY Measurements, as of weight or distance
        ORDINAL “first”, “second”
        CARDINAL Numerals that do not fall under another typ
        """

        doc = self.nlp(data)
        entities = set()

        label_list = ['PERSON', 'NORP', 'FACILITY', 'ORGANIZATION', 'GPE', 'LOCATION', 'PRODUCT',
                      'EVENT', 'WORK_OF_ART', 'LAW', 'LANGUAGE']
        for ent in doc.ents:
            if ent.label_ not in label_list:
                continue

            entities.add(ent.text)
        return list(entities)


class TextSummarization:
    def __init__(self):
        self.tokenizer = AutoTokenizer.from_pretrained("sshleifer/distilbart-cnn-12-6")
        self.model = AutoModelForSeq2SeqLM.from_pretrained("sshleifer/distilbart-cnn-12-6")

    @staticmethod
    def clean_html(raw_html):
        cleanr = re.compile('<.*?>')
        cleantext = re.sub(cleanr, '', raw_html)
        return cleantext

    def summarize(self, data):
        inputs = self.tokenizer.encode("summarize: " + data, return_tensors="pt", truncation=True, max_length=1024)
        outputs = self.model.generate(inputs, max_length=250, min_length=450, length_penalty=2.0, num_beams=4,
                                      early_stopping=True)
        generated = self.tokenizer.decode(outputs[0])
        generated = self.clean_html(generated)

        return generated


if __name__ == "__main__":
    data = """
    Hey everybody in this video we're gonna talk about topic modeling so what is topic modeling essentially
    it's a way or a suite of techniques to identify latent themes in a corpus a corpus being a group of
    documents and documents could be anything from say newspaper articles to tweets to really any kind
    of text you want to study so we might use topic modeling for a lot of different reasons but the
    people who developed topic modeling like to think of it as kind of amplified reading so the idea
    here is that it's a technique for seeing large themes in giant groups of texts that you could never
    possibly read yourself now despite the intentions of the original kind of developers of topic models
    topic models have come to be used for identifying variables or features and social science
    models as well let's start at the beginning let's go back to one of the first big papers about topic
    modeling and that was a paper on so called latent dearest lay allocation so Lda as it's known
    was first applied by david bleh computer scientist now at columbia who was interested to see
    whether a computer could be trained using a variety of kind of Bayesian learning to detect themes in scientific abstracts from the journal science so basically they read in he and his team read in a hundred years of abstracts from science and they were trying to see if they could build an algorithm that could say sort biology from chemistry and chemistry from neuroscience and so on and so forth so this figure is kind of a schematic to give you some general intuition about how a topic model works now the first thing that's a little just disappointing to a lot of people is that the topic model doesn't tell you how many topics there might be in your corpus instead you have to tell the topic model that so in this hypothetical case suppose there's four topics suppose we were looking for four topics probably there's many many more in hundred years of the journal science but just for the sake of argument let's say there's four topics well in this schematic the four topics are pictured in yellow pink green and blue and what you see in those kind of panels on the left of this slide is the top words associated with each of those topics so just like topic modeling doesn't tell you how many topics there is it gonna be it also doesn't name them for you it would be great if it told you this is the chemistry topic and this is the biology topic instead the output of a topic model is two things first it's a list of words associated with each topic with high probability and then it's an assignment of each document two topics now something that's a little interesting and unusual about topic modeling compared to the techniques that came to Florida is it's a mixed membership model so if you're familiar with cluster analysis you'll know that there's something called hard clustering which means an algorithm such as the k-means algorithm which will sort a matrix into hard clusters so each observation can belong to one and only one cluster so you know say we're trying to say we created our document term matrix which we learned about in the basic text analysis class we'll look at one of those in just a minute but that's the matrix representation of a corpus and then we just applied a cluster analysis to it like k-means we would have each document in this case each scientific abstract have to belong to one unique cluster now what Blaine and his colleagues realized which is kind of clever was that you know there's a lot of in between this and actually when we force things into a single category we lose a lot of that information now they certainly weren't the first people to develop mixed membership models but they were among the first to think about it in the realm of text analysis and natural language processing so what does that mean a mixed membership model it means that every document has a little bit of resemblance to every topic so it may be the case that a topic is very likely to be associated with say a chemistry topic and only barely associated with a genetics topic but we can think of each document in this way as a mixture of different topics and so that's what's going on in the kind of arrows on the right of this diagram and the histogram that's showing you the distribution of probabilities associated with each topic being present in the text but you're also seeing how the topic model is learning the assignment of documents to topics and words to topics in an iterative Bayesian process now typically we use in LD a something like a naive Bayes classifier or a Gibbs sampler and basically we begin by randomly assigning every document some degree of membership into each of the K topics that we feed the algorithm K always refers to the number of topics and topic modeling and so then across each turn we we in our kind of bag of words where we're simply you have this matrix that counts the presences and absences of different words and we increasingly create a better fit with that model and then once we reach a point where our model is converged or we're satisfied that it's not going to get much better we stop and we take a look at the results so here are some examples of what the results might look like a list of top words associated with each topic the same word by the way it may appear in multiple topics because it's a mixed membership model and then a group of documents and their assignments to the topics so let's try it out we're going to use the topic models packaged in R as well as the text mining package and we're gonna use some data that comes with the topic models package it's called The Associated Press dataset the AP as it's known is a large media organization in the United States and it generously donated a bunch of its news articles to people doing natural language processing some years ago and it's become kind of a popular data set for running topping models and all sorts of other kind of natural language processing analyses so the first thing I want you to see is what the data looks like what we have in this package is not actually the full-text of the documents themselves instead we have what I described earlier the document term matrix and again if you don't know what that means yet go back and re-watch the basic text analysis video where I introduced that in detail but just briefly a document term matrix is a representation of a corpus in matrix format the rows are the documents and the columns are any word that appears in the entire corpus next the cells of the matrix count the number of times each word appears in each document and this is the typical data structure we use for topic modeling so remember that takes quite a bit of work to get to a document term matrix you have to take a lot of text pre-processing steps you may be dealing with character encoding issues if none of those terms make sense to you again go back and check out the basic text analysis video here's how you run your first topic model we're gonna use a function called Lda we pass the document term matrix called associated press to it and then the second argument is very important k equals 10 that's just a random guess of how many topics we might find in this corpus of based articles it's probably wrong usually the first run of a topic model is very wrong and we have to exploit either explore a range of different values of K or use some goodness-of-fit measures we'll learn about in a few minutes to help us kind of make a better choice but above all you should always use human intuition and human validation of the codes rather than allowing the computer to do it for you because we're simply not at the point where human classification has been surpassed by unsupervised algorithms at least in my view finally in this function you'll see a control parameter this is important if you want the results of your topic model to be reproducible if you want to be able to send them to your friends and they get the same results to wow you have leaned for if you're all doing topic modeling right anyways these are what this is what my friends do I guess okay so never mind that you want to set a seed you want to set a random number so that the results are reproduced the same time each way and that's the last thing going on in this function here so now we can use the tidy text package to kind of clean up some of the results and some deep liar basically what we're doing is we're pulling out the beta's those are the probabilities of each word being associated with each topic and we are grouping them we're counting the top ten by topic and we're creating a new data set that has each topic and the top ten words associated with that topic in terms of their probability of being associated with that topic with a ton of ggplot a lot of aesthetics renaming some stuff flipping the coordinates labeling some stuff we come up with this graph and what this graph shows you is the top ten words associated with each of our 10 topics and they're each colored differently so if you kind of check these out closely you'll see there's one that's kind of about the economy and another that's kind of about the Soviet Union but then there's another one that just literally seems like random words right and that's not uncommon in a first run through a topic model typically we've either underestimated the number of topics or overestimate the number of topics and so again we'd have to rerun them so in this case we would probably want to try a larger number to try to tease out whether there are actually in fact multiple topics hidden within that single strange topic that seems to just combine a lot of really random words the real challenge with any kind of unsupervised text analysis any type of cluster analysis really more broadly speaking is that we're often reading tea leaves this is not my phrase as the phrase of Jonathan Chang a data scientist at Facebook was a nice paper on validating topic models and he compares it to the Japanese ceremony of reading tea leaves so if you don't know about this essentially during a tea ceremony once the tea is finished the leaves of the tea arranged themselves on the bottom of the glass and one then looks into it in order to see some type of pattern or some type of you know something that might happen in the future and so obviously the way the t's fall is somewhat arbitrary and then we as humans attach meaning to it we're really good at that right too good at that sometimes and so this is the real danger with topic modeling we may drop a certain value of K into the model see something we like drop a different model in say something we don't like and we don't really have a good solid process for figuring out exactly what our process should be there's some interesting work by Lauren Nelson an assistant professor at Northeastern who's done a little bit of work on combining qualitative and quantitative methods I've also written a little bit about this but the short version of the story is nothing will replace high quality human validation and some combination of a kind of systematic approach where at least try a range of different values and inspect them carefully both by looking at the distribution of the top words but also by reading the documents themselves and really trying to see if the topics that the model is showing you exist actually coherent to something that is meaningful another tricky thing is kind of what is a topic so it was pretty straightforward when we were talking about scientific abstracts right we all know what chemistry is we all know what genetics is but can a topic model kind of find things like text about democracy or text about feminism the answer is kind of it depends on how you would define those things if we can be reasonably assured that the co-occurrence of words is reliably associated with a term like democracy then maybe it could on the other hand if there's something very nuanced about the style of language something about the taken-for-granted nassif the text or some obscure references to democratic theorists that aren't reliably alongside words like say Parliament we may really struggle to measure things like that so really topics are kind of in the eye of the beholder and that's another reason why it's challenging to use them and fit them into models and convinced other social scientists that they measure something that's real in the world so just as you're getting into it resist the temptation to just let the kind of the algorithm do all the work for you you still need to tell the rest of us what your model means a really useful tool for doing that and improving upon some of the kind of core weaknesses of topic modeling is a wonderful technique called structural topic modeling this is a package developed by molly roberts brandon stewart and dustin tingly three political scientists and they have just a great art package it combines a lot of the steps we've already learned in our basic text analysis class like text pre-processing and it also runs a structural topic model a structural topic bottle it's very similar to an Lda model except that exploits metadata about a document in order to improve the classification of topics within a corpus let me give you an example if we have say a hundred years of newspaper data and we have some articles about the war on drugs which happened in the 1980s in the united states and we have other articles about world war ii right the term war is being used in both contexts but obviously the term war in nineteen fifty and something very different shortly after the world the Second World War then it did in the nineteen eighties and so what this means is that something as simple as time could really condition the way that word should be grouped with each other and so basically the structural topic modeling allows you to include those those covariates as predictors of the probabilities in the assignment of documents to topics so the actual modeling is really nice we get a little more specificity there but then there where the model really excels in my view is that it really helps the reader of the topic model kind of interpreted both by finding exemplary quotes visualizing and there's some great user kind of add-on visualizations we're going to talk about in just a minute as well and then also just generally browsing the data and really deep way and then also has some great functions for using goodness-of-fit measures to help you determine the value of K so let's take a look at the package in a little more detail I'm gonna draw heavily on the vignette by the authors of the package themselves so here we're reading in a data set of political blogs from 2008 I put it on a Google Drive and we're simply reading it in with read CSV here so if you browse the document you'll see that each row each row is a different document or blog post and there's a little bit of information on the date in which the blog was was blog post was produced the name of the blog and then whether or not the blog is liberal or conservative that variable in this data set is called rating and so basically what we did in a previous video on basic text analysis when we did things like stemming and removing numbers and punctuation we can do all of that and one nifty step with STM using the text processor function like we are here we have to specify where the metadata are if they're not in the same data set as the documents for us here they are we have a single data set that includes both the documents and the metadata so we can just specify it as follows next we have to create a few different objects which the package is going to use to help us browse the data later specifically we're creating something we're gonna run the models on a vocabulary of all the different words that appear and the metadata itself and the full text of the documents here's how we run our first structural topic model so the core function in the package is STM we specify where that full text of the documents are we specified the vocabulary that's the unique words that ever appear we give it a value of K again that's the number of topics and then in the next argument we do something really important which is we specify whether or how metadata should be used to predict topic assignment which is called prevalence here so here we're using some kind of regression style notation to see that we want to use both a smooth value of the day variable to kind of account for shifts over time and then also the rating variable which again is going to say whether the blog is either liberal or conservative then we once again are kind of setting some parameters to control how rapidly the model will proceed and when you run this you'll begin to see that iterations and for each iteration you'll see a list of topics and top words which is kind of nice because you can get a sense of how rapidly the model is working if you set verbose equals true here we've shut it off just to save some space in our our console a really nice simple function is this plot function in STM here we're plotting the results of this model with ten topics and here you can see the first topics not very impressive you know like one think who knows what that is but at the same time there's clearly topics about the Obama campaign about the McCain campaign about taxes about education about the Iraq war and so we can see at beginning it's doing pretty well but likely we need far more than ten topics just like we needed more than ten topics in our Associated Press example so let's figure out how we do that well the first thing we might want to do is just kind of inspect a few of the topics this fine thoughts function is really handy for doing that this allows you to pass the output of the topic model the full text and then look for in this case two examples of topic number three in it well it will pull up passages of those documents that load really high such I'll help you try to figure out what that topic actually is another really handy function is the search K function here we run a structural topic model again just like we did before but instead of specifying a single value of K we can specify a range so here we've specified a range of sorry this should say 10 : 30 which would be every value between 10 and 30 and so once this runs and this will take a while to run because depending on the size of your corpus you know topic mom could take anywhere from a few minutes to 30 minutes to run depending on the speed of your computer as well but then once we plot the output of search K we see various goodness-of-fit measures that further help us try to interpret the output now knowing the authors of some of these goodness-of-fit measures I can tell you that they will tell you that these goodness-of-fit measures are no substitute for human validation of the model but one one way that I found them very useful to work with is to specify a likely range of where the best fit for the topic model exists you know maybe it's between 25 and 30 in this model so the other nice thing about the SDM package is you can work with metadata so once you've used the metadata predicted to Britney to the topics you can also then study how the topics are associated with the metadata which is often what we want to do in the first place of a topic modeling very often we just want to answer descriptive questions what were people talking about in 2008 we're liberals talking about different things than conservatives when did each side start talking about different things these are the types of things that we can answer really nicely with the STM package so here we're going to just plot the effects by a covariant and we're gonna plot them specifically by the liberal conservative rating and time and here's our plot with some base our graphics and here you can see topics 3 & 5 tend to lean more conservative and topic 9 tends to lean more liberal we can do the same thing with time here we're going to output the the distribution of one topic over time and you know the more pieces of metadata we have the moral we can play with with these types of visualizations one word of warning is just because you have lots of metadata it doesn't mean it should all go in the model just like you can over fit any kind of model imagine if you dumped every piece of information about the author you had in the model and when and where and how the author was writing eventually your topic model might start to predict the author herself instead of something more generic that could be looked at across documents so you know you should purposely include metadata that you think might be a associated with the distribution of topics or metadata that you want to look at as predictors of the topics themselves there's a really nice function for analyzing Lda results either from the topic models package we're also from STM thanks to a handy helper called the - LD a vis package but what this neat LD ava's tool does is create an interactive HTML widget which allows you to look at and inspect not only the top words associated with each document that's the kind of panel on the right here but also plot the distribution of the topics both their size and their relationship to each other in a principal component space so what that means is those circles on the left side the circles that are closer to each other have similar content similar probabilities of words appearing in them and the larger ones are more prevalent in the corpus so what you can do with this is start to see well if there's two topics they're really close to each other maybe they're the same topic and maybe you should choose a lower value of K on the other hand if LD Ava shows you a giant kind of death star of a topic right then you need to increase your value of K because likely that giant death star is actually a lot of little smaller topics sorry for the geek reference okay one big problem with topic modeling is that it really does best when it has at least you know a couple hundred words in a document and let's say you want to model tweets my experience is many of the existing models Lda STM just don't do very well with tweets so in the polarisation lab we've been lurking on a new solution to this it's called STL DAC and it's a really neat technique the core difference is that rather than assuming a mixture model where each in this case tweet could belong to multiple topics we rather assume that each tweet can only be assigned to one topic we think that's reasonable because most tweets aren't very long in the first place and can't really contain more than one topic so then each user has a distribution over topics in all words in the tweet are a draw from this name distribution over words so basically it's very similar to a topic model except with one added feature which is that we also cluster users along the way so it's neat you get both a topic distribution and a clustering of users which we often want anyways if for example we were gonna try to group Republicans and and and Democrats who are all talking about something and we get a really high-quality classification that beats according to our analysis just about every other model on short texts out there so the repo is linked here we're going to add a link to the paper to go ahead and check that out if you're working with Twitter data so the limitations of topic models by now are hopefully pretty clear they're pretty powerful tools for amplified reading for reading more than you possibly ever could on your own the danger is if you really don't have any idea what's in that corpus you may read tea leaves right you may start finding patterns that aren't actually patterns themselves you may also you know find false negatives right you may have been looking for something and you know just because a topic model didn't find similar co-occurrence of words it doesn't mean that it doesn't exist in the text and again maybe you would have been better served by a simple dictionary analysis lastly topic models assume that word order doesn't matter it's a so-called bag-of-words assumption that means that we dump everything into a document term matrix and then we just kind of assume that where the begins in the beginning of the document at the end of the document are equally kind of important in terms of their co-presidents near each other and often that's a reasonable assumption but sometimes it's not and that assumption is kind of one thing that we'll revisit when we get into text networks or if you get to check out my other material on word embeddings you'll see in that case we use word sequences to try to solve that problem as well so I hope you enjoy this video and I'll see you in the next one [Music] you
    """

    tk = TokenClassification()

    topics = tk.tokenize(data)
